{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import random\n",
    "from argparse import Namespace\n",
    "from datetime import datetime\n",
    "from os.path import expanduser\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "from maskedvae.model.models import ModelGLVM\n",
    "from maskedvae.model.networks import GLVM_VAE\n",
    "from maskedvae.utils.utils import ensure_directory, save_pickle, save_yaml\n",
    "from maskedvae.model.masks import MultipleDimGauss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up configuration and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\": 42, # random seed\n",
    "    \"epochs\": 100, # number of epochs\n",
    "    \"beta\": 1.0, # beta value for beta-vae\n",
    "    \"all_obs\": 0, # 1 for all observed data\n",
    "    \"visualise\": 0, # save visualisations during training\n",
    "    \"shorttest\": 0, # load shorter dataset for testing run setup\n",
    "    \"all\": 1, # run all methods in the list \n",
    "    \"method\": 0, # specify which method if all=0\n",
    "    \"one_impute\": 0, # impute 1 for all masked values\n",
    "    \"val_impute\": 1, # impute a specific value \n",
    "    \"mean_impute\": 0, # impute a respective mean \n",
    "    \"random_impute\": 0, # impute a random value mean \n",
    "    \"list_len\": 1, # only one data condition C, noise\n",
    "    \"task_name\": \"glvm\", # task name\n",
    "    \"exp\": \"glvm\", # experiment name wandb\n",
    "    \"offline\": 1, #1 sets wandb offline\n",
    "    \"cross_loss\": 0, # alternative training loss off\n",
    "    \"full_loss\": 0, # alternative training loss on all data off\n",
    "    \"dropout\": 0.0, # dropout to compare to simple dropout off\n",
    "}\n",
    "if config[\"offline\"]:\n",
    "    os.environ[\"WANDB_MODE\"] = \"offline\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_directory = \"../../data/\"\n",
    "run_directory = \"../../runs/\"\n",
    "ensure_directory(run_directory)\n",
    "\n",
    "with open('../../configs/glvm/fit_config.yml', \"r\") as f:\n",
    "    data_conf = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "# update config with data_conf\n",
    "# the second overwrites the first - comman line will beat config\n",
    "args_dict = {**data_conf.__dict__, **config}\n",
    "args = Namespace(**args_dict)\n",
    "\n",
    "assert np.abs(args.fraction_full - 1 / (args.unique_masks + 1)) <= 0.05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set seeds to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup torch and seeds\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_num_threads(1)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "method_handle = copy.copy(args.method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training, validation and test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  9000 valid  1000 test  1000\n"
     ]
    }
   ],
   "source": [
    "c_id = 0 \n",
    "dataset_train = torch.load(\n",
    "    f\"{data_directory:s}/glvm/20_dim_1_dim/data_train_{c_id:d}.pt\"\n",
    ")\n",
    "dataset_valid = torch.load(\n",
    "    f\"{data_directory:s}/glvm/20_dim_1_dim/data_valid_{c_id:d}.pt\"\n",
    ")\n",
    "dataset_test = torch.load(\n",
    "    f\"{data_directory:s}/glvm/20_dim_1_dim/data_test_{c_id:d}.pt\"\n",
    ")\n",
    "\n",
    "args.C = dataset_train.C\n",
    "args.d = dataset_train.d\n",
    "args.z_prior = np.stack((np.zeros(args.z_dim), np.ones(args.z_dim)))\n",
    "args.noise = dataset_train.noise\n",
    "\n",
    "print(\"train \", len(dataset_train), \"valid \", len(dataset_valid), \"test \", len(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the running modes: masked vs naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    "    \"zero_imputation_mask_concatenated_encoder_only\", # masked with all_obs = 0\n",
    "    \"zero_imputation\", # naive with all_obs = 1\n",
    "]\n",
    "# short names for WandB\n",
    "method_short = [\n",
    "    \"enc-mask-\",  # masekd\n",
    "    \"zero-\",    # naive\n",
    "]\n",
    "meth_labels = ['masked', 'naive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logs = {\n",
    "    method: {\n",
    "        \"elbo\": [],\n",
    "        \"kl\": [],\n",
    "        \"rec_loss\": [],\n",
    "        \"elbo_val\": [],\n",
    "        \"kl_val\": [],\n",
    "        \"rec_loss_val\": [],\n",
    "        \"masked_rec_loss\": [],\n",
    "        \"masked_rec_loss_val\": [],\n",
    "        \"observed_mse\": [],\n",
    "        \"masked_mse\": [],\n",
    "        \"time_stamp_val\": [],\n",
    "    }\n",
    "    for method in methods\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the Mask generator + run paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.ts = datetime.now().strftime(\"%d%m%Y_%H%M\" \"%S\")  # more easily readable\n",
    "\n",
    "# Specify the masking generator\n",
    "args.generator = MultipleDimGauss\n",
    "# Specify which non linearity to use for the networks\n",
    "nonlin_mapping = {0: nn.Identity(), 1: nn.ELU(), 2: nn.ReLU(), 3: nn.Sigmoid()}\n",
    "args.nonlin_fn = nonlin_mapping.get(args.nonlin, nn.Identity())\n",
    "    \n",
    "# ensure either one or mean imputation if both zero standard zero imputation\n",
    "args.mean_impute = not args.one_impute and args.mean_impute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xdims_20_C_1.23_sig_0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked model...\n",
      "Namespace(C=array([[ 1.23315865],\n",
      "       [-1.1715279 ],\n",
      "       [ 0.94545997],\n",
      "       [-1.09916162],\n",
      "       [-1.1621336 ],\n",
      "       [-1.02799144],\n",
      "       [ 1.12655116],\n",
      "       [-1.11085485],\n",
      "       [ 1.10042914],\n",
      "       [-1.08253998],\n",
      "       [-1.14330262],\n",
      "       [ 1.22030374],\n",
      "       [-1.00349343],\n",
      "       [ 1.20282741],\n",
      "       [ 1.12286301],\n",
      "       [-1.14451376],\n",
      "       [ 0.98633978],\n",
      "       [ 1.11351369],\n",
      "       [ 1.2484537 ],\n",
      "       [-0.99201951]]), across_channels=0, added_std=0.001, all=1, all_obs=0, baseline=1.0, baselined=False, batch_size=1000, beta=1.0, binarise=1, class_scale=0.0, combination='regular', cross_loss=0, d=array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), dataparams='z1d_negCorr_diff_error', datasamples=10000, dropout=0.0, earlystop=2500, epochs=100, eval_plot_only_all_obs=0, exp='masked', fig_dir='../../runs/wandb/offline-run-20250223_141723-vtw5prdm/files/figs', fig_root='../../runs/', fraction_full=0.25, freeze_after_training=0, freeze_decoder=1, full_loss=0, generator=<class 'maskedvae.model.masks.MultipleDimGauss'>, imputeoff=1, latent_size=1, latentclassification=0, learning_rate=0.001, list_len=1, loss_type='regular', masked=True, mean_impute=0, method='zero_imputation_mask_concatenated_encoder_only', n_data_dims=20, n_hidden=60, n_masked_vals=10, n_samples=128, noise=array([[0.45592079],\n",
      "       [0.58343981],\n",
      "       [0.90058431],\n",
      "       [0.76911918],\n",
      "       [1.2043483 ],\n",
      "       [0.83677052],\n",
      "       [0.77645999],\n",
      "       [0.65102501],\n",
      "       [0.75978269],\n",
      "       [1.05980115],\n",
      "       [1.02119656],\n",
      "       [1.08661559],\n",
      "       [0.62085787],\n",
      "       [0.4945024 ],\n",
      "       [0.55301233],\n",
      "       [0.83365408],\n",
      "       [1.5668288 ],\n",
      "       [0.7889871 ],\n",
      "       [0.89210305],\n",
      "       [0.30544752]]), nonlin=2, nonlin_fn=ReLU(), offline=1, one_impute=0, only_obs=0, pass_mask=True, posterior_reg=0, print_every=100, project_name='glvm_in_maskedvae', random_impute=0, random_masks=0, run_test=0, sampling=1, seed=42, shorttest=0, shrinkmask=0, specified_masks=0, store_model=0, task_name='glvm', test_batch_size=4000, ts='23022025_141722', uncertainty=1, unique_masks=3, val_impute=1, valid_batch_size=4000, visualise=0, warmup_range=30, watchmodel=0, wholedata=0, x_dim=20, z_dim=1, z_prior=array([[0.],\n",
      "       [1.]]))\n",
      "Run model\n",
      "Gaussian Neg. log lik loss\n",
      "GLVM_VAE\n",
      "Traineable parameters:  7442\n",
      " ---------- begin model fit ---------------\n",
      "zero_imputation_mask_concatenated_encoder_only\n",
      "Epoch 00/100 Batch 0000/8, Loss   33.0798\n",
      "Epoch 00/100 Batch 0008/8, Loss   27.6509\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  0\n",
      "1000\n",
      "validation  0.007650852203369141\n",
      "Epoch 00/100 Batch 0008/8, Validation Loss   38.1638\n",
      "Epoch 01/100 Batch 0000/8, Loss   33.4732\n",
      "Epoch 01/100 Batch 0008/8, Loss   21.7407\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  1\n",
      "1000\n",
      "validation  0.007663249969482422\n",
      "Epoch 01/100 Batch 0008/8, Validation Loss   29.3628\n",
      "Epoch 02/100 Batch 0000/8, Loss   20.4875\n",
      "Epoch 02/100 Batch 0008/8, Loss   18.0538\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  2\n",
      "1000\n",
      "validation  0.009129047393798828\n",
      "Epoch 02/100 Batch 0008/8, Validation Loss   22.1744\n",
      "Epoch 03/100 Batch 0000/8, Loss   21.5084\n",
      "Epoch 03/100 Batch 0008/8, Loss   19.8039\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  3\n",
      "1000\n",
      "validation  0.007468223571777344\n",
      "Epoch 03/100 Batch 0008/8, Validation Loss   13.2412\n",
      "Epoch 04/100 Batch 0000/8, Loss   11.9931\n",
      "Epoch 04/100 Batch 0008/8, Loss    6.4321\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  4\n",
      "1000\n",
      "validation  0.007515430450439453\n",
      "Epoch 04/100 Batch 0008/8, Validation Loss    6.6613\n",
      "Epoch 05/100 Batch 0000/8, Loss    6.0849\n",
      "Epoch 05/100 Batch 0008/8, Loss    3.1788\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  5\n",
      "1000\n",
      "validation  0.007544517517089844\n",
      "Epoch 05/100 Batch 0008/8, Validation Loss    5.3973\n",
      "Epoch 06/100 Batch 0000/8, Loss    6.7517\n",
      "Epoch 06/100 Batch 0008/8, Loss    5.3723\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  6\n",
      "1000\n",
      "validation  0.008352041244506836\n",
      "Epoch 06/100 Batch 0008/8, Validation Loss    5.9852\n",
      "Epoch 07/100 Batch 0000/8, Loss    5.9660\n",
      "Epoch 07/100 Batch 0008/8, Loss    4.1481\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  7\n",
      "1000\n",
      "validation  0.009257793426513672\n",
      "Epoch 07/100 Batch 0008/8, Validation Loss    3.4959\n",
      "Epoch 08/100 Batch 0000/8, Loss    2.7793\n",
      "Epoch 08/100 Batch 0008/8, Loss    3.4819\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  8\n",
      "1000\n",
      "validation  0.007620334625244141\n",
      "Epoch 08/100 Batch 0008/8, Validation Loss    4.6063\n",
      "Epoch 09/100 Batch 0000/8, Loss    2.6232\n",
      "Epoch 09/100 Batch 0008/8, Loss    3.3437\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  9\n",
      "1000\n",
      "validation  0.007706880569458008\n",
      "Epoch 09/100 Batch 0008/8, Validation Loss    4.8513\n",
      "Epoch 10/100 Batch 0000/8, Loss    3.5111\n",
      "Epoch 10/100 Batch 0008/8, Loss    3.5820\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  10\n",
      "1000\n",
      "validation  0.007407188415527344\n",
      "Epoch 10/100 Batch 0008/8, Validation Loss    3.9783\n",
      "Epoch 11/100 Batch 0000/8, Loss    6.1298\n",
      "Epoch 11/100 Batch 0008/8, Loss    5.1904\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  11\n",
      "1000\n",
      "validation  0.007501125335693359\n",
      "Epoch 11/100 Batch 0008/8, Validation Loss    5.1281\n",
      "Epoch 12/100 Batch 0000/8, Loss    5.0283\n",
      "Epoch 12/100 Batch 0008/8, Loss    3.7994\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  12\n",
      "1000\n",
      "validation  0.008045434951782227\n",
      "Epoch 12/100 Batch 0008/8, Validation Loss    4.0160\n",
      "Epoch 13/100 Batch 0000/8, Loss    3.7762\n",
      "Epoch 13/100 Batch 0008/8, Loss    4.1880\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  13\n",
      "1000\n",
      "validation  0.008650779724121094\n",
      "Epoch 13/100 Batch 0008/8, Validation Loss    4.7798\n",
      "Epoch 14/100 Batch 0000/8, Loss    3.2607\n",
      "Epoch 14/100 Batch 0008/8, Loss    6.3417\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  14\n",
      "1000\n",
      "validation  0.0076181888580322266\n",
      "Epoch 14/100 Batch 0008/8, Validation Loss    4.9476\n",
      "Epoch 15/100 Batch 0000/8, Loss    4.5050\n",
      "Epoch 15/100 Batch 0008/8, Loss    2.8792\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  15\n",
      "1000\n",
      "validation  0.0075016021728515625\n",
      "Epoch 15/100 Batch 0008/8, Validation Loss    3.9020\n",
      "Epoch 16/100 Batch 0000/8, Loss    2.9548\n",
      "Epoch 16/100 Batch 0008/8, Loss    6.9397\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  16\n",
      "1000\n",
      "validation  0.007399320602416992\n",
      "Epoch 16/100 Batch 0008/8, Validation Loss    4.5289\n",
      "Epoch 17/100 Batch 0000/8, Loss    2.9466\n",
      "Epoch 17/100 Batch 0008/8, Loss    3.0295\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  17\n",
      "1000\n",
      "validation  0.0075664520263671875\n",
      "Epoch 17/100 Batch 0008/8, Validation Loss    3.9095\n",
      "Epoch 18/100 Batch 0000/8, Loss    4.0897\n",
      "Epoch 18/100 Batch 0008/8, Loss    2.8892\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  18\n",
      "1000\n",
      "validation  0.007525444030761719\n",
      "Epoch 18/100 Batch 0008/8, Validation Loss    4.5415\n",
      "Epoch 19/100 Batch 0000/8, Loss    3.0098\n",
      "Epoch 19/100 Batch 0008/8, Loss    4.0882\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  19\n",
      "1000\n",
      "validation  0.0075304508209228516\n",
      "Epoch 19/100 Batch 0008/8, Validation Loss    4.6262\n",
      "Epoch 20/100 Batch 0000/8, Loss    3.1827\n",
      "Epoch 20/100 Batch 0008/8, Loss    6.6698\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  20\n",
      "1000\n",
      "validation  0.0075452327728271484\n",
      "Epoch 20/100 Batch 0008/8, Validation Loss    4.0739\n",
      "Epoch 21/100 Batch 0000/8, Loss    3.2124\n",
      "Epoch 21/100 Batch 0008/8, Loss    3.3811\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  21\n",
      "1000\n",
      "validation  0.007429599761962891\n",
      "Epoch 21/100 Batch 0008/8, Validation Loss    4.1142\n",
      "Epoch 22/100 Batch 0000/8, Loss    6.7303\n",
      "Epoch 22/100 Batch 0008/8, Loss    6.8598\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  22\n",
      "1000\n",
      "validation  0.007658481597900391\n",
      "Epoch 22/100 Batch 0008/8, Validation Loss    4.1602\n",
      "Epoch 23/100 Batch 0000/8, Loss    6.9272\n",
      "Epoch 23/100 Batch 0008/8, Loss    3.2990\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  23\n",
      "1000\n",
      "validation  0.007630825042724609\n",
      "Epoch 23/100 Batch 0008/8, Validation Loss    4.8623\n",
      "Epoch 24/100 Batch 0000/8, Loss    6.8423\n",
      "Epoch 24/100 Batch 0008/8, Loss    3.4491\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  24\n",
      "1000\n",
      "validation  0.0074732303619384766\n",
      "Epoch 24/100 Batch 0008/8, Validation Loss    4.2704\n",
      "Epoch 25/100 Batch 0000/8, Loss    3.5169\n",
      "Epoch 25/100 Batch 0008/8, Loss    3.5893\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  25\n",
      "1000\n",
      "validation  0.007666587829589844\n",
      "Epoch 25/100 Batch 0008/8, Validation Loss    4.0238\n",
      "Epoch 26/100 Batch 0000/8, Loss    3.7270\n",
      "Epoch 26/100 Batch 0008/8, Loss    3.4814\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  26\n",
      "1000\n",
      "validation  0.007966995239257812\n",
      "Epoch 26/100 Batch 0008/8, Validation Loss    4.4039\n",
      "Epoch 27/100 Batch 0000/8, Loss    3.6689\n",
      "Epoch 27/100 Batch 0008/8, Loss    4.3554\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  27\n",
      "1000\n",
      "validation  0.0075452327728271484\n",
      "Epoch 27/100 Batch 0008/8, Validation Loss    4.8695\n",
      "Epoch 28/100 Batch 0000/8, Loss    3.6011\n",
      "Epoch 28/100 Batch 0008/8, Loss    4.4617\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  28\n",
      "1000\n",
      "validation  0.0076067447662353516\n",
      "Epoch 28/100 Batch 0008/8, Validation Loss    4.2706\n",
      "Epoch 29/100 Batch 0000/8, Loss    4.5645\n",
      "Epoch 29/100 Batch 0008/8, Loss    7.0594\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  29\n",
      "1000\n",
      "validation  0.007769584655761719\n",
      "Epoch 29/100 Batch 0008/8, Validation Loss    4.3925\n",
      "Epoch 30/100 Batch 0000/8, Loss    3.5776\n",
      "Epoch 30/100 Batch 0008/8, Loss    7.1328\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  30\n",
      "1000\n",
      "validation  0.007667064666748047\n",
      "Epoch 30/100 Batch 0008/8, Validation Loss    4.9424\n",
      "Epoch 31/100 Batch 0000/8, Loss    4.4669\n",
      "Epoch 31/100 Batch 0008/8, Loss    3.7017\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  31\n",
      "1000\n",
      "validation  0.012024402618408203\n",
      "Epoch 31/100 Batch 0008/8, Validation Loss    4.3945\n",
      "Epoch 32/100 Batch 0000/8, Loss    3.6985\n",
      "Epoch 32/100 Batch 0008/8, Loss    4.4337\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  32\n",
      "1000\n",
      "validation  0.007850885391235352\n",
      "Epoch 32/100 Batch 0008/8, Validation Loss    4.9921\n",
      "Epoch 33/100 Batch 0000/8, Loss    6.7707\n",
      "Epoch 33/100 Batch 0008/8, Loss    3.6321\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  33\n",
      "1000\n",
      "validation  0.008600234985351562\n",
      "Epoch 33/100 Batch 0008/8, Validation Loss    4.3322\n",
      "Epoch 34/100 Batch 0000/8, Loss    4.2537\n",
      "Epoch 34/100 Batch 0008/8, Loss    3.5107\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  34\n",
      "1000\n",
      "validation  0.008494138717651367\n",
      "Epoch 34/100 Batch 0008/8, Validation Loss    4.2492\n",
      "Epoch 35/100 Batch 0000/8, Loss    7.0360\n",
      "Epoch 35/100 Batch 0008/8, Loss    4.4794\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  35\n",
      "1000\n",
      "validation  0.007615804672241211\n",
      "Epoch 35/100 Batch 0008/8, Validation Loss    4.2493\n",
      "Epoch 36/100 Batch 0000/8, Loss    3.4537\n",
      "Epoch 36/100 Batch 0008/8, Loss    4.3830\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  36\n",
      "1000\n",
      "validation  0.007472038269042969\n",
      "Epoch 36/100 Batch 0008/8, Validation Loss    4.4049\n",
      "Epoch 37/100 Batch 0000/8, Loss    3.5192\n",
      "Epoch 37/100 Batch 0008/8, Loss    3.5567\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  37\n",
      "1000\n",
      "validation  0.0075418949127197266\n",
      "Epoch 37/100 Batch 0008/8, Validation Loss    4.3396\n",
      "Epoch 38/100 Batch 0000/8, Loss    3.4370\n",
      "Epoch 38/100 Batch 0008/8, Loss    7.0341\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  38\n",
      "1000\n",
      "validation  0.007681369781494141\n",
      "Epoch 38/100 Batch 0008/8, Validation Loss    4.4212\n",
      "Epoch 39/100 Batch 0000/8, Loss    3.4395\n",
      "Epoch 39/100 Batch 0008/8, Loss    3.7206\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  39\n",
      "1000\n",
      "validation  0.007497310638427734\n",
      "Epoch 39/100 Batch 0008/8, Validation Loss    4.2342\n",
      "Epoch 40/100 Batch 0000/8, Loss    3.5326\n",
      "Epoch 40/100 Batch 0008/8, Loss    3.4698\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  40\n",
      "1000\n",
      "validation  0.00753474235534668\n",
      "Epoch 40/100 Batch 0008/8, Validation Loss    4.3469\n",
      "Epoch 41/100 Batch 0000/8, Loss    3.6348\n",
      "Epoch 41/100 Batch 0008/8, Loss    3.6855\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  41\n",
      "1000\n",
      "validation  0.008304834365844727\n",
      "Epoch 41/100 Batch 0008/8, Validation Loss    4.2329\n",
      "Epoch 42/100 Batch 0000/8, Loss    6.9501\n",
      "Epoch 42/100 Batch 0008/8, Loss    4.3890\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  42\n",
      "1000\n",
      "validation  0.1353905200958252\n",
      "Epoch 42/100 Batch 0008/8, Validation Loss    4.2183\n",
      "Epoch 43/100 Batch 0000/8, Loss    4.4109\n",
      "Epoch 43/100 Batch 0008/8, Loss    3.7099\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  43\n",
      "1000\n",
      "validation  0.007641792297363281\n",
      "Epoch 43/100 Batch 0008/8, Validation Loss    4.3434\n",
      "Epoch 44/100 Batch 0000/8, Loss    6.8879\n",
      "Epoch 44/100 Batch 0008/8, Loss    6.6608\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  44\n",
      "1000\n",
      "validation  0.0076141357421875\n",
      "Epoch 44/100 Batch 0008/8, Validation Loss    4.9383\n",
      "Epoch 45/100 Batch 0000/8, Loss    4.4515\n",
      "Epoch 45/100 Batch 0008/8, Loss    6.8244\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  45\n",
      "1000\n",
      "validation  0.007516384124755859\n",
      "Epoch 45/100 Batch 0008/8, Validation Loss    4.2138\n",
      "Epoch 46/100 Batch 0000/8, Loss    3.6208\n",
      "Epoch 46/100 Batch 0008/8, Loss    4.4532\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  46\n",
      "1000\n",
      "validation  0.007482051849365234\n",
      "Epoch 46/100 Batch 0008/8, Validation Loss    5.0114\n",
      "Epoch 47/100 Batch 0000/8, Loss    4.4918\n",
      "Epoch 47/100 Batch 0008/8, Loss    4.4384\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  47\n",
      "1000\n",
      "validation  0.0076253414154052734\n",
      "Epoch 47/100 Batch 0008/8, Validation Loss    4.8236\n",
      "Epoch 48/100 Batch 0000/8, Loss    3.6715\n",
      "Epoch 48/100 Batch 0008/8, Loss    3.6305\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  48\n",
      "1000\n",
      "validation  0.007805347442626953\n",
      "Epoch 48/100 Batch 0008/8, Validation Loss    4.3700\n",
      "Epoch 49/100 Batch 0000/8, Loss    3.3716\n",
      "Epoch 49/100 Batch 0008/8, Loss    6.7437\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  49\n",
      "1000\n",
      "validation  0.007860898971557617\n",
      "Epoch 49/100 Batch 0008/8, Validation Loss    4.8870\n",
      "Epoch 50/100 Batch 0000/8, Loss    3.6696\n",
      "Epoch 50/100 Batch 0008/8, Loss    4.3743\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  50\n",
      "1000\n",
      "validation  0.008952856063842773\n",
      "Epoch 50/100 Batch 0008/8, Validation Loss    4.3815\n",
      "Epoch 51/100 Batch 0000/8, Loss    3.5308\n",
      "Epoch 51/100 Batch 0008/8, Loss    3.5335\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  51\n",
      "1000\n",
      "validation  0.007860660552978516\n",
      "Epoch 51/100 Batch 0008/8, Validation Loss    4.8634\n",
      "Epoch 52/100 Batch 0000/8, Loss    6.8238\n",
      "Epoch 52/100 Batch 0008/8, Loss    4.3805\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  52\n",
      "1000\n",
      "validation  0.010316133499145508\n",
      "Epoch 52/100 Batch 0008/8, Validation Loss    4.1682\n",
      "Epoch 53/100 Batch 0000/8, Loss    6.7507\n",
      "Epoch 53/100 Batch 0008/8, Loss    6.6738\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  53\n",
      "1000\n",
      "validation  0.007892131805419922\n",
      "Epoch 53/100 Batch 0008/8, Validation Loss    4.8497\n",
      "Epoch 54/100 Batch 0000/8, Loss    4.2915\n",
      "Epoch 54/100 Batch 0008/8, Loss    3.5618\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  54\n",
      "1000\n",
      "validation  0.008718729019165039\n",
      "Epoch 54/100 Batch 0008/8, Validation Loss    4.3657\n",
      "Epoch 55/100 Batch 0000/8, Loss    6.6757\n",
      "Epoch 55/100 Batch 0008/8, Loss    3.5724\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  55\n",
      "1000\n",
      "validation  0.007561683654785156\n",
      "Epoch 55/100 Batch 0008/8, Validation Loss    4.1997\n",
      "Epoch 56/100 Batch 0000/8, Loss    4.3735\n",
      "Epoch 56/100 Batch 0008/8, Loss    4.4290\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  56\n",
      "1000\n",
      "validation  0.008051872253417969\n",
      "Epoch 56/100 Batch 0008/8, Validation Loss    4.3344\n",
      "Epoch 57/100 Batch 0000/8, Loss    4.5212\n",
      "Epoch 57/100 Batch 0008/8, Loss    3.6222\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  57\n",
      "1000\n",
      "validation  0.0077550411224365234\n",
      "Epoch 57/100 Batch 0008/8, Validation Loss    4.4609\n",
      "Epoch 58/100 Batch 0000/8, Loss    3.4566\n",
      "Epoch 58/100 Batch 0008/8, Loss    3.6114\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  58\n",
      "1000\n",
      "validation  0.008312225341796875\n",
      "Epoch 58/100 Batch 0008/8, Validation Loss    5.0353\n",
      "Epoch 59/100 Batch 0000/8, Loss    3.6597\n",
      "Epoch 59/100 Batch 0008/8, Loss    3.7587\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  59\n",
      "1000\n",
      "validation  0.007796525955200195\n",
      "Epoch 59/100 Batch 0008/8, Validation Loss    4.1792\n",
      "Epoch 60/100 Batch 0000/8, Loss    4.4158\n",
      "Epoch 60/100 Batch 0008/8, Loss    4.3872\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  60\n",
      "1000\n",
      "validation  0.00768733024597168\n",
      "Epoch 60/100 Batch 0008/8, Validation Loss    4.3155\n",
      "Epoch 61/100 Batch 0000/8, Loss    3.5053\n",
      "Epoch 61/100 Batch 0008/8, Loss    4.4791\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  61\n",
      "1000\n",
      "validation  0.007684946060180664\n",
      "Epoch 61/100 Batch 0008/8, Validation Loss    4.2713\n",
      "Epoch 62/100 Batch 0000/8, Loss    6.7161\n",
      "Epoch 62/100 Batch 0008/8, Loss    3.6734\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  62\n",
      "1000\n",
      "validation  0.0077588558197021484\n",
      "Epoch 62/100 Batch 0008/8, Validation Loss    4.8269\n",
      "Epoch 63/100 Batch 0000/8, Loss    4.4531\n",
      "Epoch 63/100 Batch 0008/8, Loss    3.5248\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  63\n",
      "1000\n",
      "validation  0.007505178451538086\n",
      "Epoch 63/100 Batch 0008/8, Validation Loss    4.4499\n",
      "Epoch 64/100 Batch 0000/8, Loss    4.4807\n",
      "Epoch 64/100 Batch 0008/8, Loss    3.5570\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  64\n",
      "1000\n",
      "validation  0.007634639739990234\n",
      "Epoch 64/100 Batch 0008/8, Validation Loss    4.8787\n",
      "Epoch 65/100 Batch 0000/8, Loss    6.8647\n",
      "Epoch 65/100 Batch 0008/8, Loss    3.4154\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  65\n",
      "1000\n",
      "validation  0.007731199264526367\n",
      "Epoch 65/100 Batch 0008/8, Validation Loss    4.2746\n",
      "Epoch 66/100 Batch 0000/8, Loss    6.8547\n",
      "Epoch 66/100 Batch 0008/8, Loss    4.1997\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  66\n",
      "1000\n",
      "validation  0.007540225982666016\n",
      "Epoch 66/100 Batch 0008/8, Validation Loss    4.3761\n",
      "Epoch 67/100 Batch 0000/8, Loss    4.4767\n",
      "Epoch 67/100 Batch 0008/8, Loss    4.7439\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  67\n",
      "1000\n",
      "validation  0.007977008819580078\n",
      "Epoch 67/100 Batch 0008/8, Validation Loss    4.5828\n",
      "Epoch 68/100 Batch 0000/8, Loss    6.8211\n",
      "Epoch 68/100 Batch 0008/8, Loss    6.7629\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  68\n",
      "1000\n",
      "validation  0.0077953338623046875\n",
      "Epoch 68/100 Batch 0008/8, Validation Loss    4.4710\n",
      "Epoch 69/100 Batch 0000/8, Loss    3.7394\n",
      "Epoch 69/100 Batch 0008/8, Loss    3.5799\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  69\n",
      "1000\n",
      "validation  0.007721424102783203\n",
      "Epoch 69/100 Batch 0008/8, Validation Loss    4.8989\n",
      "Epoch 70/100 Batch 0000/8, Loss    6.7850\n",
      "Epoch 70/100 Batch 0008/8, Loss    6.8240\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  70\n",
      "1000\n",
      "validation  0.008100509643554688\n",
      "Epoch 70/100 Batch 0008/8, Validation Loss    4.8143\n",
      "Epoch 71/100 Batch 0000/8, Loss    3.3373\n",
      "Epoch 71/100 Batch 0008/8, Loss    6.6300\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  71\n",
      "1000\n",
      "validation  0.007839202880859375\n",
      "Epoch 71/100 Batch 0008/8, Validation Loss    4.9626\n",
      "Epoch 72/100 Batch 0000/8, Loss    3.7613\n",
      "Epoch 72/100 Batch 0008/8, Loss    3.5241\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  72\n",
      "1000\n",
      "validation  0.008044719696044922\n",
      "Epoch 72/100 Batch 0008/8, Validation Loss    4.1215\n",
      "Epoch 73/100 Batch 0000/8, Loss    3.5903\n",
      "Epoch 73/100 Batch 0008/8, Loss    4.4170\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  73\n",
      "1000\n",
      "validation  0.007699728012084961\n",
      "Epoch 73/100 Batch 0008/8, Validation Loss    4.3484\n",
      "Epoch 74/100 Batch 0000/8, Loss    4.3292\n",
      "Epoch 74/100 Batch 0008/8, Loss    6.8397\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  74\n",
      "1000\n",
      "validation  0.007916450500488281\n",
      "Epoch 74/100 Batch 0008/8, Validation Loss    4.3517\n",
      "Epoch 75/100 Batch 0000/8, Loss    3.3648\n",
      "Epoch 75/100 Batch 0008/8, Loss    4.5154\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  75\n",
      "1000\n",
      "validation  0.008153676986694336\n",
      "Epoch 75/100 Batch 0008/8, Validation Loss    4.2064\n",
      "Epoch 76/100 Batch 0000/8, Loss    3.5394\n",
      "Epoch 76/100 Batch 0008/8, Loss    6.7335\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  76\n",
      "1000\n",
      "validation  0.007737398147583008\n",
      "Epoch 76/100 Batch 0008/8, Validation Loss    4.3588\n",
      "Epoch 77/100 Batch 0000/8, Loss    3.5472\n",
      "Epoch 77/100 Batch 0008/8, Loss    4.2423\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  77\n",
      "1000\n",
      "validation  0.008461475372314453\n",
      "Epoch 77/100 Batch 0008/8, Validation Loss    4.8428\n",
      "Epoch 78/100 Batch 0000/8, Loss    4.2045\n",
      "Epoch 78/100 Batch 0008/8, Loss    6.7136\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  78\n",
      "1000\n",
      "validation  0.008907556533813477\n",
      "Epoch 78/100 Batch 0008/8, Validation Loss    4.3565\n",
      "Epoch 79/100 Batch 0000/8, Loss    3.4834\n",
      "Epoch 79/100 Batch 0008/8, Loss    6.6616\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  79\n",
      "1000\n",
      "validation  0.008419036865234375\n",
      "Epoch 79/100 Batch 0008/8, Validation Loss    4.7620\n",
      "Epoch 80/100 Batch 0000/8, Loss    4.2597\n",
      "Epoch 80/100 Batch 0008/8, Loss    4.2656\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  80\n",
      "1000\n",
      "validation  0.007878780364990234\n",
      "Epoch 80/100 Batch 0008/8, Validation Loss    4.2317\n",
      "Epoch 81/100 Batch 0000/8, Loss    3.5337\n",
      "Epoch 81/100 Batch 0008/8, Loss    3.5165\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  81\n",
      "1000\n",
      "validation  0.00777125358581543\n",
      "Epoch 81/100 Batch 0008/8, Validation Loss    4.2139\n",
      "Epoch 82/100 Batch 0000/8, Loss    3.4208\n",
      "Epoch 82/100 Batch 0008/8, Loss    3.6008\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  82\n",
      "1000\n",
      "validation  0.007681369781494141\n",
      "Epoch 82/100 Batch 0008/8, Validation Loss    4.1254\n",
      "Epoch 83/100 Batch 0000/8, Loss    3.4245\n",
      "Epoch 83/100 Batch 0008/8, Loss    3.8071\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  83\n",
      "1000\n",
      "validation  0.008419990539550781\n",
      "Epoch 83/100 Batch 0008/8, Validation Loss    4.2720\n",
      "Epoch 84/100 Batch 0000/8, Loss    4.3685\n",
      "Epoch 84/100 Batch 0008/8, Loss    3.4877\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  84\n",
      "1000\n",
      "validation  0.007840394973754883\n",
      "Epoch 84/100 Batch 0008/8, Validation Loss    4.1095\n",
      "Epoch 85/100 Batch 0000/8, Loss    6.6457\n",
      "Epoch 85/100 Batch 0008/8, Loss    3.5292\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  85\n",
      "1000\n",
      "validation  0.007816314697265625\n",
      "Epoch 85/100 Batch 0008/8, Validation Loss    4.1935\n",
      "Epoch 86/100 Batch 0000/8, Loss    3.3933\n",
      "Epoch 86/100 Batch 0008/8, Loss    3.6064\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  86\n",
      "1000\n",
      "validation  0.00775456428527832\n",
      "Epoch 86/100 Batch 0008/8, Validation Loss    4.2676\n",
      "Epoch 87/100 Batch 0000/8, Loss    6.5167\n",
      "Epoch 87/100 Batch 0008/8, Loss    4.5709\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  87\n",
      "1000\n",
      "validation  0.007637500762939453\n",
      "Epoch 87/100 Batch 0008/8, Validation Loss    4.0469\n",
      "Epoch 88/100 Batch 0000/8, Loss    6.5808\n",
      "Epoch 88/100 Batch 0008/8, Loss    3.3934\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  88\n",
      "1000\n",
      "validation  0.007643699645996094\n",
      "Epoch 88/100 Batch 0008/8, Validation Loss    4.8134\n",
      "Epoch 89/100 Batch 0000/8, Loss    6.4833\n",
      "Epoch 89/100 Batch 0008/8, Loss    4.2925\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  89\n",
      "1000\n",
      "validation  0.0078084468841552734\n",
      "Epoch 89/100 Batch 0008/8, Validation Loss    4.1080\n",
      "Epoch 90/100 Batch 0000/8, Loss    3.7341\n",
      "Epoch 90/100 Batch 0008/8, Loss    3.5527\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  90\n",
      "1000\n",
      "validation  0.007634878158569336\n",
      "Epoch 90/100 Batch 0008/8, Validation Loss    4.2874\n",
      "Epoch 91/100 Batch 0000/8, Loss    3.4456\n",
      "Epoch 91/100 Batch 0008/8, Loss    6.6134\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  91\n",
      "1000\n",
      "validation  0.007821798324584961\n",
      "Epoch 91/100 Batch 0008/8, Validation Loss    4.1445\n",
      "Epoch 92/100 Batch 0000/8, Loss    4.3885\n",
      "Epoch 92/100 Batch 0008/8, Loss    4.2349\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  92\n",
      "1000\n",
      "validation  0.007654428482055664\n",
      "Epoch 92/100 Batch 0008/8, Validation Loss    4.7536\n",
      "Epoch 93/100 Batch 0000/8, Loss    4.2794\n",
      "Epoch 93/100 Batch 0008/8, Loss    3.5067\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  93\n",
      "1000\n",
      "validation  0.0076024532318115234\n",
      "Epoch 93/100 Batch 0008/8, Validation Loss    4.7534\n",
      "Epoch 94/100 Batch 0000/8, Loss    4.4624\n",
      "Epoch 94/100 Batch 0008/8, Loss    3.4764\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  94\n",
      "1000\n",
      "validation  0.007708311080932617\n",
      "Epoch 94/100 Batch 0008/8, Validation Loss    4.1120\n",
      "Epoch 95/100 Batch 0000/8, Loss    6.5998\n",
      "Epoch 95/100 Batch 0008/8, Loss    3.4477\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  95\n",
      "1000\n",
      "validation  0.007752656936645508\n",
      "Epoch 95/100 Batch 0008/8, Validation Loss    4.0542\n",
      "Epoch 96/100 Batch 0000/8, Loss    6.5485\n",
      "Epoch 96/100 Batch 0008/8, Loss    4.3690\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  96\n",
      "1000\n",
      "validation  0.007642507553100586\n",
      "Epoch 96/100 Batch 0008/8, Validation Loss    4.1152\n",
      "Epoch 97/100 Batch 0000/8, Loss    6.6698\n",
      "Epoch 97/100 Batch 0008/8, Loss    3.4654\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  97\n",
      "1000\n",
      "validation  0.0077397823333740234\n",
      "Epoch 97/100 Batch 0008/8, Validation Loss    4.8547\n",
      "Epoch 98/100 Batch 0000/8, Loss    3.5436\n",
      "Epoch 98/100 Batch 0008/8, Loss    3.4046\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  98\n",
      "1000\n",
      "validation  0.00780034065246582\n",
      "Epoch 98/100 Batch 0008/8, Validation Loss    4.8322\n",
      "Epoch 99/100 Batch 0000/8, Loss    6.5701\n",
      "Epoch 99/100 Batch 0008/8, Loss    4.5231\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  99\n",
      "1000\n",
      "validation  0.007695436477661133\n",
      "Epoch 99/100 Batch 0008/8, Validation Loss    4.2761\n",
      " ---------- end model fit ---------------\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beta</td><td>▁▁▂▃▃▄▅▅▆▇▇█████████████████████████████</td></tr><tr><td>elbo</td><td>█▆▂▁▂▃▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>elbo_val</td><td>█▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>kl</td><td>▁▁▃▅██▆▅▅▆▅▅▅▅▆▅▅▅▅▅▅▅▅▅▅▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>kl_val</td><td>▁▂▅▇██▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>masked_rec_loss</td><td>█▅▂▂▁▁▂▂▂▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>masked_rec_loss_val</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>post var full</td><td>█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>post var mask</td><td>█▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>rec mean MSE</td><td>▁█▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>rec_loss</td><td>█▆▂▂▂▃▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>rec_loss_val</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beta</td><td>1.0</td></tr><tr><td>elbo</td><td>4.52306</td></tr><tr><td>elbo_val</td><td>4.27607</td></tr><tr><td>kl</td><td>1.78038</td></tr><tr><td>kl_val</td><td>1.73603</td></tr><tr><td>masked_rec_loss</td><td>2.78526</td></tr><tr><td>masked_rec_loss_val</td><td>2.7148</td></tr><tr><td>post var full</td><td>0.01733</td></tr><tr><td>post var mask</td><td>0.03161</td></tr><tr><td>rec mean MSE</td><td>0.00389</td></tr><tr><td>rec_loss</td><td>2.74268</td></tr><tr><td>rec_loss_val</td><td>2.54003</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync ../../runs/wandb/offline-run-20250223_141723-vtw5prdm<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../../runs/wandb/offline-run-20250223_141723-vtw5prdm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xdims_20_C_1.23_sig_0.46\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked model...\n",
      "Namespace(C=array([[ 1.23315865],\n",
      "       [-1.1715279 ],\n",
      "       [ 0.94545997],\n",
      "       [-1.09916162],\n",
      "       [-1.1621336 ],\n",
      "       [-1.02799144],\n",
      "       [ 1.12655116],\n",
      "       [-1.11085485],\n",
      "       [ 1.10042914],\n",
      "       [-1.08253998],\n",
      "       [-1.14330262],\n",
      "       [ 1.22030374],\n",
      "       [-1.00349343],\n",
      "       [ 1.20282741],\n",
      "       [ 1.12286301],\n",
      "       [-1.14451376],\n",
      "       [ 0.98633978],\n",
      "       [ 1.11351369],\n",
      "       [ 1.2484537 ],\n",
      "       [-0.99201951]]), across_channels=0, added_std=0.001, all=1, all_obs=1, baseline=1.0, baselined=False, batch_size=1000, beta=1.0, betastep=0.03333333333333333, binarise=1, class_scale=0.0, combination='regular', cross_loss=0, d=array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), dataparams='z1d_negCorr_diff_error', datasamples=10000, dropout=0.0, earlystop=2500, epochs=100, eval_plot_only_all_obs=0, exp='naive', fig_dir='../../runs/wandb/offline-run-20250223_141742-iiwzyuw5/files/figs', fig_root='../../runs/', fraction_full=0.25, freeze_after_training=0, freeze_decoder=1, full_loss=0, generator=<class 'maskedvae.model.masks.MultipleDimGauss'>, imputeoff=1, latent_size=1, latentclassification=0, learning_rate=0.001, list_len=1, loss_type='regular', masked=True, mean_impute=0, method='zero_imputation', n_data_dims=20, n_hidden=60, n_masked_vals=10, n_parameters=7442, n_samples=128, noise=array([[0.45592079],\n",
      "       [0.58343981],\n",
      "       [0.90058431],\n",
      "       [0.76911918],\n",
      "       [1.2043483 ],\n",
      "       [0.83677052],\n",
      "       [0.77645999],\n",
      "       [0.65102501],\n",
      "       [0.75978269],\n",
      "       [1.05980115],\n",
      "       [1.02119656],\n",
      "       [1.08661559],\n",
      "       [0.62085787],\n",
      "       [0.4945024 ],\n",
      "       [0.55301233],\n",
      "       [0.83365408],\n",
      "       [1.5668288 ],\n",
      "       [0.7889871 ],\n",
      "       [0.89210305],\n",
      "       [0.30544752]]), nonlin=2, nonlin_fn=ReLU(), offline=1, one_impute=0, only_obs=0, pass_mask=True, pass_mask_decoder=False, posterior_reg=0, print_every=100, project_name='glvm_in_maskedvae', random_impute=0, random_masks=0, run_test=0, sampling=1, seed=42, shorttest=0, shrinkmask=0, specified_masks=0, store_model=0, task_name='glvm', test_batch_size=4000, ts='23022025_141722', uncertainty=1, unique_masks=3, val_impute=1, valid_batch_size=4000, visualise=0, warmup_range=30, watchmodel=0, wholedata=0, x_dim=20, z_dim=1, z_prior=array([[0.],\n",
      "       [1.]]))\n",
      "Run model\n",
      "Gaussian Neg. log lik loss\n",
      "GLVM_VAE\n",
      "Traineable parameters:  7442\n",
      " ---------- begin model fit ---------------\n",
      "zero_imputation\n",
      "Epoch 00/100 Batch 0000/8, Loss   55.9589\n",
      "Epoch 00/100 Batch 0008/8, Loss   42.6950\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  0\n",
      "1000\n",
      "validation  0.009835004806518555\n",
      "Epoch 00/100 Batch 0008/8, Validation Loss   42.5800\n",
      "Epoch 01/100 Batch 0000/8, Loss   40.8473\n",
      "Epoch 01/100 Batch 0008/8, Loss   32.6693\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  1\n",
      "1000\n",
      "validation  0.007965326309204102\n",
      "Epoch 01/100 Batch 0008/8, Validation Loss   30.1202\n",
      "Epoch 02/100 Batch 0000/8, Loss   28.8158\n",
      "Epoch 02/100 Batch 0008/8, Loss   20.9284\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  2\n",
      "1000\n",
      "validation  0.007885456085205078\n",
      "Epoch 02/100 Batch 0008/8, Validation Loss   20.5821\n",
      "Epoch 03/100 Batch 0000/8, Loss   21.0403\n",
      "Epoch 03/100 Batch 0008/8, Loss   11.8660\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  3\n",
      "1000\n",
      "validation  0.008716106414794922\n",
      "Epoch 03/100 Batch 0008/8, Validation Loss   11.1747\n",
      "Epoch 04/100 Batch 0000/8, Loss   10.3548\n",
      "Epoch 04/100 Batch 0008/8, Loss    6.2279\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  4\n",
      "1000\n",
      "validation  0.007828712463378906\n",
      "Epoch 04/100 Batch 0008/8, Validation Loss    5.8171\n",
      "Epoch 05/100 Batch 0000/8, Loss    5.9516\n",
      "Epoch 05/100 Batch 0008/8, Loss    5.0461\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  5\n",
      "1000\n",
      "validation  0.007927894592285156\n",
      "Epoch 05/100 Batch 0008/8, Validation Loss    5.0602\n",
      "Epoch 06/100 Batch 0000/8, Loss    5.2856\n",
      "Epoch 06/100 Batch 0008/8, Loss    4.9834\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  6\n",
      "1000\n",
      "validation  0.008870601654052734\n",
      "Epoch 06/100 Batch 0008/8, Validation Loss    5.0613\n",
      "Epoch 07/100 Batch 0000/8, Loss    5.1566\n",
      "Epoch 07/100 Batch 0008/8, Loss    5.0577\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  7\n",
      "1000\n",
      "validation  0.008216142654418945\n",
      "Epoch 07/100 Batch 0008/8, Validation Loss    5.1266\n",
      "Epoch 08/100 Batch 0000/8, Loss    5.0639\n",
      "Epoch 08/100 Batch 0008/8, Loss    5.2926\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  8\n",
      "1000\n",
      "validation  0.007868051528930664\n",
      "Epoch 08/100 Batch 0008/8, Validation Loss    5.1734\n",
      "Epoch 09/100 Batch 0000/8, Loss    5.1824\n",
      "Epoch 09/100 Batch 0008/8, Loss    5.2166\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  9\n",
      "1000\n",
      "validation  0.008066892623901367\n",
      "Epoch 09/100 Batch 0008/8, Validation Loss    5.2055\n",
      "Epoch 10/100 Batch 0000/8, Loss    5.3314\n",
      "Epoch 10/100 Batch 0008/8, Loss    5.2068\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  10\n",
      "1000\n",
      "validation  0.008007287979125977\n",
      "Epoch 10/100 Batch 0008/8, Validation Loss    5.2350\n",
      "Epoch 11/100 Batch 0000/8, Loss    5.3753\n",
      "Epoch 11/100 Batch 0008/8, Loss    5.2779\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  11\n",
      "1000\n",
      "validation  0.008064508438110352\n",
      "Epoch 11/100 Batch 0008/8, Validation Loss    5.3154\n",
      "Epoch 12/100 Batch 0000/8, Loss    5.6465\n",
      "Epoch 12/100 Batch 0008/8, Loss    5.4589\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  12\n",
      "1000\n",
      "validation  0.008315801620483398\n",
      "Epoch 12/100 Batch 0008/8, Validation Loss    5.3411\n",
      "Epoch 13/100 Batch 0000/8, Loss    5.5556\n",
      "Epoch 13/100 Batch 0008/8, Loss    5.4559\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  13\n",
      "1000\n",
      "validation  0.00794219970703125\n",
      "Epoch 13/100 Batch 0008/8, Validation Loss    5.3903\n",
      "Epoch 14/100 Batch 0000/8, Loss    5.5677\n",
      "Epoch 14/100 Batch 0008/8, Loss    5.4883\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  14\n",
      "1000\n",
      "validation  0.008267641067504883\n",
      "Epoch 14/100 Batch 0008/8, Validation Loss    5.4909\n",
      "Epoch 15/100 Batch 0000/8, Loss    5.5627\n",
      "Epoch 15/100 Batch 0008/8, Loss    5.5042\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  15\n",
      "1000\n",
      "validation  0.009145736694335938\n",
      "Epoch 15/100 Batch 0008/8, Validation Loss    5.5750\n",
      "Epoch 16/100 Batch 0000/8, Loss    5.6295\n",
      "Epoch 16/100 Batch 0008/8, Loss    5.6127\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  16\n",
      "1000\n",
      "validation  0.007991790771484375\n",
      "Epoch 16/100 Batch 0008/8, Validation Loss    5.6296\n",
      "Epoch 17/100 Batch 0000/8, Loss    5.7376\n",
      "Epoch 17/100 Batch 0008/8, Loss    5.7493\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  17\n",
      "1000\n",
      "validation  0.008024215698242188\n",
      "Epoch 17/100 Batch 0008/8, Validation Loss    5.7294\n",
      "Epoch 18/100 Batch 0000/8, Loss    5.9203\n",
      "Epoch 18/100 Batch 0008/8, Loss    5.8394\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  18\n",
      "1000\n",
      "validation  0.008376359939575195\n",
      "Epoch 18/100 Batch 0008/8, Validation Loss    5.7923\n",
      "Epoch 19/100 Batch 0000/8, Loss    5.9173\n",
      "Epoch 19/100 Batch 0008/8, Loss    5.7117\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  19\n",
      "1000\n",
      "validation  0.007948637008666992\n",
      "Epoch 19/100 Batch 0008/8, Validation Loss    5.8201\n",
      "Epoch 20/100 Batch 0000/8, Loss    5.8900\n",
      "Epoch 20/100 Batch 0008/8, Loss    5.8863\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  20\n",
      "1000\n",
      "validation  0.008394241333007812\n",
      "Epoch 20/100 Batch 0008/8, Validation Loss    5.9074\n",
      "Epoch 21/100 Batch 0000/8, Loss    6.0340\n",
      "Epoch 21/100 Batch 0008/8, Loss    6.0440\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  21\n",
      "1000\n",
      "validation  0.008387327194213867\n",
      "Epoch 21/100 Batch 0008/8, Validation Loss    5.9633\n",
      "Epoch 22/100 Batch 0000/8, Loss    5.9538\n",
      "Epoch 22/100 Batch 0008/8, Loss    5.8696\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  22\n",
      "1000\n",
      "validation  0.007825851440429688\n",
      "Epoch 22/100 Batch 0008/8, Validation Loss    6.0569\n",
      "Epoch 23/100 Batch 0000/8, Loss    6.1284\n",
      "Epoch 23/100 Batch 0008/8, Loss    6.0497\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  23\n",
      "1000\n",
      "validation  0.008301496505737305\n",
      "Epoch 23/100 Batch 0008/8, Validation Loss    6.1199\n",
      "Epoch 24/100 Batch 0000/8, Loss    6.1083\n",
      "Epoch 24/100 Batch 0008/8, Loss    6.3409\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  24\n",
      "1000\n",
      "validation  0.008383035659790039\n",
      "Epoch 24/100 Batch 0008/8, Validation Loss    6.1804\n",
      "Epoch 25/100 Batch 0000/8, Loss    6.2177\n",
      "Epoch 25/100 Batch 0008/8, Loss    6.2060\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  25\n",
      "1000\n",
      "validation  0.008510112762451172\n",
      "Epoch 25/100 Batch 0008/8, Validation Loss    6.2349\n",
      "Epoch 26/100 Batch 0000/8, Loss    6.4445\n",
      "Epoch 26/100 Batch 0008/8, Loss    6.2684\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  26\n",
      "1000\n",
      "validation  0.008164167404174805\n",
      "Epoch 26/100 Batch 0008/8, Validation Loss    6.3516\n",
      "Epoch 27/100 Batch 0000/8, Loss    6.5282\n",
      "Epoch 27/100 Batch 0008/8, Loss    6.4188\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  27\n",
      "1000\n",
      "validation  0.008147239685058594\n",
      "Epoch 27/100 Batch 0008/8, Validation Loss    6.4018\n",
      "Epoch 28/100 Batch 0000/8, Loss    6.4932\n",
      "Epoch 28/100 Batch 0008/8, Loss    6.4121\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  28\n",
      "1000\n",
      "validation  0.007872581481933594\n",
      "Epoch 28/100 Batch 0008/8, Validation Loss    6.4858\n",
      "Epoch 29/100 Batch 0000/8, Loss    6.4347\n",
      "Epoch 29/100 Batch 0008/8, Loss    6.6242\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  29\n",
      "1000\n",
      "validation  0.007985353469848633\n",
      "Epoch 29/100 Batch 0008/8, Validation Loss    6.4946\n",
      "Epoch 30/100 Batch 0000/8, Loss    6.6482\n",
      "Epoch 30/100 Batch 0008/8, Loss    6.5588\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  30\n",
      "1000\n",
      "validation  0.008697509765625\n",
      "Epoch 30/100 Batch 0008/8, Validation Loss    6.5172\n",
      "Epoch 31/100 Batch 0000/8, Loss    6.6376\n",
      "Epoch 31/100 Batch 0008/8, Loss    6.4786\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  31\n",
      "1000\n",
      "validation  0.008111715316772461\n",
      "Epoch 31/100 Batch 0008/8, Validation Loss    6.4948\n",
      "Epoch 32/100 Batch 0000/8, Loss    6.4068\n",
      "Epoch 32/100 Batch 0008/8, Loss    6.5132\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  32\n",
      "1000\n",
      "validation  0.008176803588867188\n",
      "Epoch 32/100 Batch 0008/8, Validation Loss    6.4983\n",
      "Epoch 33/100 Batch 0000/8, Loss    6.4602\n",
      "Epoch 33/100 Batch 0008/8, Loss    6.6512\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  33\n",
      "1000\n",
      "validation  0.00839543342590332\n",
      "Epoch 33/100 Batch 0008/8, Validation Loss    6.5269\n",
      "Epoch 34/100 Batch 0000/8, Loss    6.5470\n",
      "Epoch 34/100 Batch 0008/8, Loss    6.5147\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  34\n",
      "1000\n",
      "validation  0.008012771606445312\n",
      "Epoch 34/100 Batch 0008/8, Validation Loss    6.5369\n",
      "Epoch 35/100 Batch 0000/8, Loss    6.6934\n",
      "Epoch 35/100 Batch 0008/8, Loss    6.4569\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  35\n",
      "1000\n",
      "validation  0.008360147476196289\n",
      "Epoch 35/100 Batch 0008/8, Validation Loss    6.5102\n",
      "Epoch 36/100 Batch 0000/8, Loss    6.4126\n",
      "Epoch 36/100 Batch 0008/8, Loss    6.4758\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  36\n",
      "1000\n",
      "validation  0.008045673370361328\n",
      "Epoch 36/100 Batch 0008/8, Validation Loss    6.4641\n",
      "Epoch 37/100 Batch 0000/8, Loss    6.5930\n",
      "Epoch 37/100 Batch 0008/8, Loss    6.6319\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  37\n",
      "1000\n",
      "validation  0.008325338363647461\n",
      "Epoch 37/100 Batch 0008/8, Validation Loss    6.5097\n",
      "Epoch 38/100 Batch 0000/8, Loss    6.6256\n",
      "Epoch 38/100 Batch 0008/8, Loss    6.2800\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  38\n",
      "1000\n",
      "validation  0.008950471878051758\n",
      "Epoch 38/100 Batch 0008/8, Validation Loss    6.4539\n",
      "Epoch 39/100 Batch 0000/8, Loss    6.5389\n",
      "Epoch 39/100 Batch 0008/8, Loss    6.5749\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  39\n",
      "1000\n",
      "validation  0.008089303970336914\n",
      "Epoch 39/100 Batch 0008/8, Validation Loss    6.4837\n",
      "Epoch 40/100 Batch 0000/8, Loss    6.5815\n",
      "Epoch 40/100 Batch 0008/8, Loss    6.6497\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  40\n",
      "1000\n",
      "validation  0.00804448127746582\n",
      "Epoch 40/100 Batch 0008/8, Validation Loss    6.4630\n",
      "Epoch 41/100 Batch 0000/8, Loss    6.5152\n",
      "Epoch 41/100 Batch 0008/8, Loss    6.5809\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  41\n",
      "1000\n",
      "validation  0.009150266647338867\n",
      "Epoch 41/100 Batch 0008/8, Validation Loss    6.4676\n",
      "Epoch 42/100 Batch 0000/8, Loss    6.5442\n",
      "Epoch 42/100 Batch 0008/8, Loss    6.4759\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  42\n",
      "1000\n",
      "validation  0.008705615997314453\n",
      "Epoch 42/100 Batch 0008/8, Validation Loss    6.5080\n",
      "Epoch 43/100 Batch 0000/8, Loss    6.5271\n",
      "Epoch 43/100 Batch 0008/8, Loss    6.4904\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  43\n",
      "1000\n",
      "validation  0.008737325668334961\n",
      "Epoch 43/100 Batch 0008/8, Validation Loss    6.5200\n",
      "Epoch 44/100 Batch 0000/8, Loss    6.5660\n",
      "Epoch 44/100 Batch 0008/8, Loss    6.7362\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  44\n",
      "1000\n",
      "validation  0.009406328201293945\n",
      "Epoch 44/100 Batch 0008/8, Validation Loss    6.5131\n",
      "Epoch 45/100 Batch 0000/8, Loss    6.4111\n",
      "Epoch 45/100 Batch 0008/8, Loss    6.5360\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  45\n",
      "1000\n",
      "validation  0.008605241775512695\n",
      "Epoch 45/100 Batch 0008/8, Validation Loss    6.4716\n",
      "Epoch 46/100 Batch 0000/8, Loss    6.5424\n",
      "Epoch 46/100 Batch 0008/8, Loss    6.5209\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  46\n",
      "1000\n",
      "validation  0.008917570114135742\n",
      "Epoch 46/100 Batch 0008/8, Validation Loss    6.4861\n",
      "Epoch 47/100 Batch 0000/8, Loss    6.2630\n",
      "Epoch 47/100 Batch 0008/8, Loss    6.6091\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  47\n",
      "1000\n",
      "validation  0.008281230926513672\n",
      "Epoch 47/100 Batch 0008/8, Validation Loss    6.4541\n",
      "Epoch 48/100 Batch 0000/8, Loss    6.5023\n",
      "Epoch 48/100 Batch 0008/8, Loss    6.6131\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  48\n",
      "1000\n",
      "validation  0.008852481842041016\n",
      "Epoch 48/100 Batch 0008/8, Validation Loss    6.4336\n",
      "Epoch 49/100 Batch 0000/8, Loss    6.5677\n",
      "Epoch 49/100 Batch 0008/8, Loss    6.4735\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  49\n",
      "1000\n",
      "validation  0.008660316467285156\n",
      "Epoch 49/100 Batch 0008/8, Validation Loss    6.4885\n",
      "Epoch 50/100 Batch 0000/8, Loss    6.4699\n",
      "Epoch 50/100 Batch 0008/8, Loss    6.5744\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  50\n",
      "1000\n",
      "validation  0.00811314582824707\n",
      "Epoch 50/100 Batch 0008/8, Validation Loss    6.4648\n",
      "Epoch 51/100 Batch 0000/8, Loss    6.3762\n",
      "Epoch 51/100 Batch 0008/8, Loss    6.5877\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  51\n",
      "1000\n",
      "validation  0.008780479431152344\n",
      "Epoch 51/100 Batch 0008/8, Validation Loss    6.4890\n",
      "Epoch 52/100 Batch 0000/8, Loss    6.5904\n",
      "Epoch 52/100 Batch 0008/8, Loss    6.4446\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  52\n",
      "1000\n",
      "validation  0.008177995681762695\n",
      "Epoch 52/100 Batch 0008/8, Validation Loss    6.4956\n",
      "Epoch 53/100 Batch 0000/8, Loss    6.4240\n",
      "Epoch 53/100 Batch 0008/8, Loss    6.3354\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  53\n",
      "1000\n",
      "validation  0.008280038833618164\n",
      "Epoch 53/100 Batch 0008/8, Validation Loss    6.4757\n",
      "Epoch 54/100 Batch 0000/8, Loss    6.3791\n",
      "Epoch 54/100 Batch 0008/8, Loss    6.3277\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  54\n",
      "1000\n",
      "validation  0.008245468139648438\n",
      "Epoch 54/100 Batch 0008/8, Validation Loss    6.4891\n",
      "Epoch 55/100 Batch 0000/8, Loss    6.5341\n",
      "Epoch 55/100 Batch 0008/8, Loss    6.6127\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  55\n",
      "1000\n",
      "validation  0.008001327514648438\n",
      "Epoch 55/100 Batch 0008/8, Validation Loss    6.4940\n",
      "Epoch 56/100 Batch 0000/8, Loss    6.4684\n",
      "Epoch 56/100 Batch 0008/8, Loss    6.5253\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  56\n",
      "1000\n",
      "validation  0.007806301116943359\n",
      "Epoch 56/100 Batch 0008/8, Validation Loss    6.4824\n",
      "Epoch 57/100 Batch 0000/8, Loss    6.5752\n",
      "Epoch 57/100 Batch 0008/8, Loss    6.5850\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  57\n",
      "1000\n",
      "validation  0.008102893829345703\n",
      "Epoch 57/100 Batch 0008/8, Validation Loss    6.4937\n",
      "Epoch 58/100 Batch 0000/8, Loss    6.4772\n",
      "Epoch 58/100 Batch 0008/8, Loss    6.5454\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  58\n",
      "1000\n",
      "validation  0.007944107055664062\n",
      "Epoch 58/100 Batch 0008/8, Validation Loss    6.4651\n",
      "Epoch 59/100 Batch 0000/8, Loss    6.4532\n",
      "Epoch 59/100 Batch 0008/8, Loss    6.3945\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  59\n",
      "1000\n",
      "validation  0.008009910583496094\n",
      "Epoch 59/100 Batch 0008/8, Validation Loss    6.4721\n",
      "Epoch 60/100 Batch 0000/8, Loss    6.4197\n",
      "Epoch 60/100 Batch 0008/8, Loss    6.4132\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  60\n",
      "1000\n",
      "validation  0.00817561149597168\n",
      "Epoch 60/100 Batch 0008/8, Validation Loss    6.5051\n",
      "Epoch 61/100 Batch 0000/8, Loss    6.6587\n",
      "Epoch 61/100 Batch 0008/8, Loss    6.4521\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  61\n",
      "1000\n",
      "validation  0.007946014404296875\n",
      "Epoch 61/100 Batch 0008/8, Validation Loss    6.4812\n",
      "Epoch 62/100 Batch 0000/8, Loss    6.5020\n",
      "Epoch 62/100 Batch 0008/8, Loss    6.6412\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  62\n",
      "1000\n",
      "validation  0.008080720901489258\n",
      "Epoch 62/100 Batch 0008/8, Validation Loss    6.4449\n",
      "Epoch 63/100 Batch 0000/8, Loss    6.3143\n",
      "Epoch 63/100 Batch 0008/8, Loss    6.6956\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  63\n",
      "1000\n",
      "validation  0.009519577026367188\n",
      "Epoch 63/100 Batch 0008/8, Validation Loss    6.5032\n",
      "Epoch 64/100 Batch 0000/8, Loss    6.4710\n",
      "Epoch 64/100 Batch 0008/8, Loss    6.5103\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  64\n",
      "1000\n",
      "validation  0.007987260818481445\n",
      "Epoch 64/100 Batch 0008/8, Validation Loss    6.4379\n",
      "Epoch 65/100 Batch 0000/8, Loss    6.4403\n",
      "Epoch 65/100 Batch 0008/8, Loss    6.5195\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  65\n",
      "1000\n",
      "validation  0.0081329345703125\n",
      "Epoch 65/100 Batch 0008/8, Validation Loss    6.4912\n",
      "Epoch 66/100 Batch 0000/8, Loss    6.4028\n",
      "Epoch 66/100 Batch 0008/8, Loss    6.4552\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  66\n",
      "1000\n",
      "validation  0.008133411407470703\n",
      "Epoch 66/100 Batch 0008/8, Validation Loss    6.4782\n",
      "Epoch 67/100 Batch 0000/8, Loss    6.5336\n",
      "Epoch 67/100 Batch 0008/8, Loss    6.4103\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  67\n",
      "1000\n",
      "validation  0.008041143417358398\n",
      "Epoch 67/100 Batch 0008/8, Validation Loss    6.4831\n",
      "Epoch 68/100 Batch 0000/8, Loss    6.3903\n",
      "Epoch 68/100 Batch 0008/8, Loss    6.4378\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  68\n",
      "1000\n",
      "validation  0.009495258331298828\n",
      "Epoch 68/100 Batch 0008/8, Validation Loss    6.4891\n",
      "Epoch 69/100 Batch 0000/8, Loss    6.5448\n",
      "Epoch 69/100 Batch 0008/8, Loss    6.4091\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  69\n",
      "1000\n",
      "validation  0.008156538009643555\n",
      "Epoch 69/100 Batch 0008/8, Validation Loss    6.4356\n",
      "Epoch 70/100 Batch 0000/8, Loss    6.4942\n",
      "Epoch 70/100 Batch 0008/8, Loss    6.3912\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  70\n",
      "1000\n",
      "validation  0.007875204086303711\n",
      "Epoch 70/100 Batch 0008/8, Validation Loss    6.4612\n",
      "Epoch 71/100 Batch 0000/8, Loss    6.3980\n",
      "Epoch 71/100 Batch 0008/8, Loss    6.5107\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  71\n",
      "1000\n",
      "validation  0.008060216903686523\n",
      "Epoch 71/100 Batch 0008/8, Validation Loss    6.4960\n",
      "Epoch 72/100 Batch 0000/8, Loss    6.5558\n",
      "Epoch 72/100 Batch 0008/8, Loss    6.3634\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  72\n",
      "1000\n",
      "validation  0.008128166198730469\n",
      "Epoch 72/100 Batch 0008/8, Validation Loss    6.4918\n",
      "Epoch 73/100 Batch 0000/8, Loss    6.5980\n",
      "Epoch 73/100 Batch 0008/8, Loss    6.3342\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  73\n",
      "1000\n",
      "validation  0.007736682891845703\n",
      "Epoch 73/100 Batch 0008/8, Validation Loss    6.4676\n",
      "Epoch 74/100 Batch 0000/8, Loss    6.4653\n",
      "Epoch 74/100 Batch 0008/8, Loss    6.6447\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  74\n",
      "1000\n",
      "validation  0.0079803466796875\n",
      "Epoch 74/100 Batch 0008/8, Validation Loss    6.4524\n",
      "Epoch 75/100 Batch 0000/8, Loss    6.5690\n",
      "Epoch 75/100 Batch 0008/8, Loss    6.4707\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  75\n",
      "1000\n",
      "validation  0.007972478866577148\n",
      "Epoch 75/100 Batch 0008/8, Validation Loss    6.4273\n",
      "Epoch 76/100 Batch 0000/8, Loss    6.4731\n",
      "Epoch 76/100 Batch 0008/8, Loss    6.5303\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  76\n",
      "1000\n",
      "validation  0.00789499282836914\n",
      "Epoch 76/100 Batch 0008/8, Validation Loss    6.4686\n",
      "Epoch 77/100 Batch 0000/8, Loss    6.4693\n",
      "Epoch 77/100 Batch 0008/8, Loss    6.6055\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  77\n",
      "1000\n",
      "validation  0.007833480834960938\n",
      "Epoch 77/100 Batch 0008/8, Validation Loss    6.4498\n",
      "Epoch 78/100 Batch 0000/8, Loss    6.4783\n",
      "Epoch 78/100 Batch 0008/8, Loss    6.4317\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  78\n",
      "1000\n",
      "validation  0.008218765258789062\n",
      "Epoch 78/100 Batch 0008/8, Validation Loss    6.5091\n",
      "Epoch 79/100 Batch 0000/8, Loss    6.5510\n",
      "Epoch 79/100 Batch 0008/8, Loss    6.3278\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  79\n",
      "1000\n",
      "validation  0.1288912296295166\n",
      "Epoch 79/100 Batch 0008/8, Validation Loss    6.5010\n",
      "Epoch 80/100 Batch 0000/8, Loss    6.5025\n",
      "Epoch 80/100 Batch 0008/8, Loss    6.6727\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  80\n",
      "1000\n",
      "validation  0.007943868637084961\n",
      "Epoch 80/100 Batch 0008/8, Validation Loss    6.4735\n",
      "Epoch 81/100 Batch 0000/8, Loss    6.4460\n",
      "Epoch 81/100 Batch 0008/8, Loss    6.5044\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  81\n",
      "1000\n",
      "validation  0.008117914199829102\n",
      "Epoch 81/100 Batch 0008/8, Validation Loss    6.4949\n",
      "Epoch 82/100 Batch 0000/8, Loss    6.6491\n",
      "Epoch 82/100 Batch 0008/8, Loss    6.4538\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  82\n",
      "1000\n",
      "validation  0.007905721664428711\n",
      "Epoch 82/100 Batch 0008/8, Validation Loss    6.4684\n",
      "Epoch 83/100 Batch 0000/8, Loss    6.5660\n",
      "Epoch 83/100 Batch 0008/8, Loss    6.4198\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  83\n",
      "1000\n",
      "validation  0.007792949676513672\n",
      "Epoch 83/100 Batch 0008/8, Validation Loss    6.4766\n",
      "Epoch 84/100 Batch 0000/8, Loss    6.5345\n",
      "Epoch 84/100 Batch 0008/8, Loss    6.5098\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  84\n",
      "1000\n",
      "validation  0.008008718490600586\n",
      "Epoch 84/100 Batch 0008/8, Validation Loss    6.4621\n",
      "Epoch 85/100 Batch 0000/8, Loss    6.3593\n",
      "Epoch 85/100 Batch 0008/8, Loss    6.4175\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  85\n",
      "1000\n",
      "validation  0.00802922248840332\n",
      "Epoch 85/100 Batch 0008/8, Validation Loss    6.4732\n",
      "Epoch 86/100 Batch 0000/8, Loss    6.4560\n",
      "Epoch 86/100 Batch 0008/8, Loss    6.5550\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  86\n",
      "1000\n",
      "validation  0.007904529571533203\n",
      "Epoch 86/100 Batch 0008/8, Validation Loss    6.4681\n",
      "Epoch 87/100 Batch 0000/8, Loss    6.5136\n",
      "Epoch 87/100 Batch 0008/8, Loss    6.4643\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  87\n",
      "1000\n",
      "validation  0.007941722869873047\n",
      "Epoch 87/100 Batch 0008/8, Validation Loss    6.4679\n",
      "Epoch 88/100 Batch 0000/8, Loss    6.4900\n",
      "Epoch 88/100 Batch 0008/8, Loss    6.4295\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  88\n",
      "1000\n",
      "validation  0.007964372634887695\n",
      "Epoch 88/100 Batch 0008/8, Validation Loss    6.4803\n",
      "Epoch 89/100 Batch 0000/8, Loss    6.6050\n",
      "Epoch 89/100 Batch 0008/8, Loss    6.4666\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  89\n",
      "1000\n",
      "validation  0.007835149765014648\n",
      "Epoch 89/100 Batch 0008/8, Validation Loss    6.4689\n",
      "Epoch 90/100 Batch 0000/8, Loss    6.5591\n",
      "Epoch 90/100 Batch 0008/8, Loss    6.4620\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  90\n",
      "1000\n",
      "validation  0.008091211318969727\n",
      "Epoch 90/100 Batch 0008/8, Validation Loss    6.5052\n",
      "Epoch 91/100 Batch 0000/8, Loss    6.5157\n",
      "Epoch 91/100 Batch 0008/8, Loss    6.5161\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  91\n",
      "1000\n",
      "validation  0.007852315902709961\n",
      "Epoch 91/100 Batch 0008/8, Validation Loss    6.4829\n",
      "Epoch 92/100 Batch 0000/8, Loss    6.5921\n",
      "Epoch 92/100 Batch 0008/8, Loss    6.4861\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  92\n",
      "1000\n",
      "validation  0.0077745914459228516\n",
      "Epoch 92/100 Batch 0008/8, Validation Loss    6.5091\n",
      "Epoch 93/100 Batch 0000/8, Loss    6.4239\n",
      "Epoch 93/100 Batch 0008/8, Loss    6.5557\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  93\n",
      "1000\n",
      "validation  0.008331298828125\n",
      "Epoch 93/100 Batch 0008/8, Validation Loss    6.4801\n",
      "Epoch 94/100 Batch 0000/8, Loss    6.4548\n",
      "Epoch 94/100 Batch 0008/8, Loss    6.4750\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  94\n",
      "1000\n",
      "validation  0.007906436920166016\n",
      "Epoch 94/100 Batch 0008/8, Validation Loss    6.4977\n",
      "Epoch 95/100 Batch 0000/8, Loss    6.5722\n",
      "Epoch 95/100 Batch 0008/8, Loss    6.6226\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  95\n",
      "1000\n",
      "validation  0.007886648178100586\n",
      "Epoch 95/100 Batch 0008/8, Validation Loss    6.4709\n",
      "Epoch 96/100 Batch 0000/8, Loss    6.5036\n",
      "Epoch 96/100 Batch 0008/8, Loss    6.5289\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  96\n",
      "1000\n",
      "validation  0.008322715759277344\n",
      "Epoch 96/100 Batch 0008/8, Validation Loss    6.4510\n",
      "Epoch 97/100 Batch 0000/8, Loss    6.4681\n",
      "Epoch 97/100 Batch 0008/8, Loss    6.5196\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  97\n",
      "1000\n",
      "validation  0.008436441421508789\n",
      "Epoch 97/100 Batch 0008/8, Validation Loss    6.4574\n",
      "Epoch 98/100 Batch 0000/8, Loss    6.4958\n",
      "Epoch 98/100 Batch 0008/8, Loss    6.3614\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  98\n",
      "1000\n",
      "validation  0.008491039276123047\n",
      "Epoch 98/100 Batch 0008/8, Validation Loss    6.4281\n",
      "Epoch 99/100 Batch 0000/8, Loss    6.5019\n",
      "Epoch 99/100 Batch 0008/8, Loss    6.3406\n",
      "---------------- Evaluation Validation ----------------  \n",
      "Epoch:  99\n",
      "1000\n",
      "validation  0.008130788803100586\n",
      "Epoch 99/100 Batch 0008/8, Validation Loss    6.4740\n",
      " ---------- end model fit ---------------\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beta</td><td>▁▁▂▃▃▄▅▅▆▇▇█████████████████████████████</td></tr><tr><td>elbo</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>elbo_val</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>kl</td><td>▁▂▅█▇▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>kl_val</td><td>▁▂▇█▇▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>masked_rec_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>masked_rec_loss_val</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>post var full</td><td>█▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>post var mask</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>rec mean MSE</td><td>█▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>rec_loss</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>rec_loss_val</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beta</td><td>1.0</td></tr><tr><td>elbo</td><td>6.34064</td></tr><tr><td>elbo_val</td><td>6.47404</td></tr><tr><td>kl</td><td>2.0311</td></tr><tr><td>kl_val</td><td>1.9832</td></tr><tr><td>masked_rec_loss</td><td>0.0</td></tr><tr><td>masked_rec_loss_val</td><td>0.0</td></tr><tr><td>post var full</td><td>0.0177</td></tr><tr><td>post var mask</td><td>0.01777</td></tr><tr><td>rec mean MSE</td><td>0.01049</td></tr><tr><td>rec_loss</td><td>4.30954</td></tr><tr><td>rec_loss_val</td><td>4.49084</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync ../../runs/wandb/offline-run-20250223_141742-iiwzyuw5<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../../runs/wandb/offline-run-20250223_141742-iiwzyuw5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, method in enumerate(methods):\n",
    "    # pass the right logs to network\n",
    "    args.method = method\n",
    "    \n",
    "    if args.method == \"zero_imputation\":\n",
    "        args.all_obs = 1\n",
    "        args.exp = \"naive\"\n",
    "    elif args.method == \"zero_imputation_mask_concatenated_encoder_only\":\n",
    "        args.all_obs = 0\n",
    "        args.exp = \"masked\"     \n",
    "           \n",
    "    name_tag = f\"xdims_{args.x_dim}_C_{args.C[0][0]:.2f}_sig_{args.noise[0][0]:.2f}\"\n",
    "\n",
    "    if args.freeze_decoder and args.loss_type != \"regular\":\n",
    "        args.loss_type = \"regular\"\n",
    "        print(\"The decoder is frozen -> switching loss type to regular...\")\n",
    "        name_tag = f\"_frozen{name_tag}\"\n",
    "    print(name_tag)\n",
    "    # Initialize the Weights and Biases (wandb) run \n",
    "    run = wandb.init(\n",
    "        project=args.project_name,\n",
    "        group=f\"{args.exp}{name_tag}\",\n",
    "        name=f\"{method_short[i]}{args.ts}\",\n",
    "        reinit=True,\n",
    "        config=args,\n",
    "        dir=run_directory\n",
    "    )\n",
    "\n",
    "    # Setup the directory for storing figures \n",
    "    figs_directory = os.path.join(wandb.run.dir, \"figs\")\n",
    "    os.makedirs(figs_directory, exist_ok=True)  # os.makedirs can create directories and won't throw an error if the directory already exists\n",
    "    args.fig_root = run_directory\n",
    "    # Update wandb configuration with the figures directory path\n",
    "    args.fig_dir = figs_directory\n",
    "    wandb.config.update({\"fig_dir\": args.fig_dir}, allow_val_change=True)\n",
    "\n",
    "    print(\"Masked model...\")\n",
    "    model = ModelGLVM(\n",
    "        args=args,\n",
    "        dataset_train=dataset_train,\n",
    "        dataset_valid=dataset_valid,\n",
    "        dataset_test=dataset_test,\n",
    "        logs=logs[method],\n",
    "        device=device,\n",
    "        inference_model=GLVM_VAE,\n",
    "        Generator=args.generator,\n",
    "        nonlin=args.nonlin_fn,\n",
    "        dropout=args.dropout,\n",
    "    )\n",
    "\n",
    "    print(\" ---------- begin model fit ---------------\")\n",
    "    print(method)\n",
    "\n",
    "    model.fit()\n",
    "    print(\" ---------- end model fit ---------------\")\n",
    "    print(\"  \")\n",
    "\n",
    "    model_path = os.path.join(model.args.fig_root, str(model.ts), model.method)\n",
    "    ensure_directory(model_path)\n",
    "\n",
    "    # save logs and args dictionary\n",
    "    save_pickle(model.logs, os.path.join(model_path, \"logs.pkl\"))\n",
    "    save_pickle(model.args, os.path.join(model_path, \"args.pkl\"))\n",
    "    save_yaml(model.args, os.path.join(model_path, \"args.yml\"))\n",
    "\n",
    "    model.train_loader = 0\n",
    "    model.test_loader = 0\n",
    "    model.validation_loader = 0\n",
    "    \n",
    "    # save the model\n",
    "    if not model.args.watchmodel:\n",
    "        model_filepath = os.path.join(model_path, \"model_end_of_training.pt\")\n",
    "        torch.save(model, model_filepath)\n",
    "        torch.save(model, os.path.join(wandb.run.dir, \"model_end_of_training.pt\"))\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "# ------------------------ end loop over methods ----------------------------------------\n",
    "\n",
    "joint_directory_path = os.path.join(model.args.fig_root, str(model.ts))\n",
    "ensure_directory(joint_directory_path)\n",
    "save_pickle(model.logs, os.path.join(joint_directory_path, \"logs.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maskedvae.plotting.plotting import plot_losses\n",
    "from maskedvae.plotting.plotting_utils import cm2inch\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=cm2inch((30, 6)))\n",
    "plot_losses(logs, methods, ax=axes[0], log='elbo', ylabel='- elbo', xlabel='iteration')\n",
    "plot_losses(logs, methods, ax=axes[1], log='elbo_val', ylabel='- elbo', xlabel='epoch')\n",
    "plot_losses(logs, methods, ax=axes[2], log='kl', ylabel='KL', xlabel='iteration')\n",
    "plot_losses(logs, methods, ax=axes[3], log='rec_loss', ylabel='reconstruction loss', xlabel='iteration')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaenbc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
